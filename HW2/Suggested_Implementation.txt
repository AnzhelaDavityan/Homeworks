I decided to use the UCB1 (Upper Confidence Bound 1) algorithm based on its strong theoretical foundation and superior performance in balancing exploration and exploitation, which are key factors for solving multi-armed bandit problems. The UCB1 algorithm selects arms based on both their estimated mean reward and the uncertainty (variance) surrounding those estimates, ensuring that the algorithm explores less-pulled arms in the early stages, while progressively exploiting the arms with higher rewards as more information is gathered. This theoretical framework helps to prevent premature settlement on suboptimal arms and maximizes rewards over time. When compared to other algorithms like EpsilonGreedy and ThompsonSampling, UCB1 outperformed both in terms of cumulative reward and average reward. Specifically, UCB1 achieved a cumulative reward of 80,079.9807, with an average reward of 4.0040, and a cumulative regret of -79.9807. In contrast, EpsilonGreedy and ThompsonSampling had lower cumulative rewards and higher cumulative regrets, with ThompsonSampling having the highest regret. These results highlight UCB1â€™s ability to maximize long-term performance while minimizing regret, making it the optimal choice for this task. The consistency and effectiveness of UCB1 in balancing exploration and exploitation further justify its use in this experiment.
